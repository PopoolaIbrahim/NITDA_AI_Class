{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vg5-QwVv8Un8"
      },
      "source": [
        "##**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFBVuHKg8Y3r"
      },
      "outputs": [],
      "source": [
        "# import fastbook\n",
        "# fastbook.setup_book()\n",
        "\n",
        "# from fastbook import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "he-YEKXE8bVX"
      },
      "outputs": [],
      "source": [
        "from fastai.text.all import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVfLCPM_aFkz"
      },
      "source": [
        "## **NLP Deep Dive: RNNs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVlaMFIke0D5"
      },
      "source": [
        "### **What is a language model?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GODrLve_aI9Z"
      },
      "source": [
        "A `language model` is a model that has been trained to guess what the next word in a text is (having read the ones before).\n",
        "\n",
        "This kind of task is called `self-supervised learning`: we do not need to give labels to our model, just feed it lots and lots of texts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jCQycVwappX"
      },
      "source": [
        "**Self-supervised learning:**  Training a model using labels that are embedded in the `independent variable`, rather than requiring external labels. \n",
        "\n",
        "For instance, training a model to predict the next word in a text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPoylUKTevlD"
      },
      "source": [
        "### **IMDB Movie Reviews**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFi93zhybKQG"
      },
      "source": [
        "**Say you want to build a language model to classify IMDb reviews:**  \n",
        "\n",
        "- You can use a language model pretrained on Wikipedia, and great results by directly fine-tuning this language model to a movie review classifier. **But with one extra step, we can do even better**\n",
        "\n",
        "- The `Wikipedia` English is slightly different from the `IMDb` English, so instead of jumping directly to the classifier, **we could fine-tune our pretrained language model to the IMDb corpus and then use that as the base for our classifier**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9IIdOARcUr-"
      },
      "source": [
        "**NOTE:**\n",
        "\n",
        "- **Even if our language model knows the basics of the language we are using in the task (e.g., our pretrained model is in English), it helps to get used to the style of the corpus we are targeting**. \n",
        "\n",
        "- **It may be more informal language, or more technical, with new words to learn or different ways of composing sentences.**\n",
        "\n",
        "- **In the case of the IMDb dataset, there will be lots of names of movie directors and actors, and often a less formal style of language than that seen in Wikipedia.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jf8rvfVJenCQ"
      },
      "source": [
        "#### **Why do this?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEBEGhNbdJu0"
      },
      "source": [
        "\n",
        "**Practically, you get better results if you fine-tune the (sequence-based) language model prior to fine-tuning the classification model.**\n",
        "\n",
        "- For instance, for the IMDb sentiment analysis task, the dataset includes 50,000 additional movie reviews that do not have any positive or negative labels attached.\n",
        "\n",
        "- There are 25,000 labeled reviews in the training set and 25,000 in the validation set, that makes 100,000 movie reviews altogether. \n",
        "\n",
        "- We can use all of these reviews to `fine-tune` the pretrained language model, which was trained only on `Wikipedia` articles; this will result in a language model that is particularly good at predicting the next word of a movie review."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5B3jIrNeAOO"
      },
      "source": [
        "**This is known as the `Universal Language Model Fine-tuning` (ULMFit) approach.**\n",
        "\n",
        "![ulmfit.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAc8AAABtCAMAAADwBRpIAAABUFBMVEX/////y2Pk5OTx16OtYwC0NQClpaUAAADh4eH7+/vu7u7/yVvw05n/xk/9+fH258n/9+r/36mfn5/p6en78uP636n/7c3/0Wb/xUyrXwCoVwCqWwCzLwDz8/P/1Ge4uLjLy8ulUACtBgDZ2dmvHQDCwsLS0tKpqan/z3VxWizIs4f14bv/8drWv5GWlpZlZWWgfz7oz50hISHKfGm5pX1/f395bFLEnEy5fki1dTmGeFv47Oj/2JD/25vy4NtsbGyejWtiV0JGRkbGcV29UzPsu1xqX0jQq4vBjVz03q+3QBfjz77Oi4Lw29YcHBw+Pj5GPzDVnZciHheolnLYuqCIbDXwv120j0bTqFKhRADftKo1KhXmxr5TQiAvLy//wTu0cSi7UjzLoXzDZ0//2IS6SSY5MydDNRrgyLJ+ZDFOPh5iTibBjVraqqQhGg3GmHByfIYWAAAS4ElEQVR4nO3d61fa2BYA8IDKgUzLeBMDwyMkmgSmVlFa0CozoLU+Kmrrs7ZSR9vR2un09v//dvc54RUIIYETPWvd7iVvPrD6695nn5MXx/2Mn/Ez/o9Djjz0L+gbU1MP/Qv6RmzuoX9Bv+DDzIJOPZp86J/QL2J/sfp/jQ8HWQWdesQsaOzxb4xmKB8MspqhU4/GWAWNPQ48ZhMUPFkFBU9WQcGTUVDsySgo9mQUFHsGmCy5xJNNUOLJJijxZDJDTU8mQU1PJkFNTxZBG54sgjY8WQRteDJYcpueDII2PRkEbXqyB9ryZA+05ckeaMuTuZLb9mQOtO3JHGjbk7UM7fBkDbTDkzXQDk/GQDs9GQPt9GQMtNOTLVCLJ1ugFk+2QC2eFMfQyMhh9aQIGh055iyeFEEnR46nv3R60svQ8OgR9An00egxZo1HtLZRBX57PGJYOemBdmtQCFqg3RoUghbo7wHqQQnUB09aoD540iq5PnhSAvXDkxKoH56UQP3wpAPqiycdUF886ZRcXzypgPrjSQXUH08qoL//AsEkKEw4ZF5kEnRycmou9pRJ0LlYLCb9HaeOOgIor6lKLiQIoZxiZIORCG3TEUDLd9s7V+OpZGJ85cdJbWqKtukIoOX64carCV1P714/P6zPxf6gLDokaNDAkq0QBEHRZJkJ0Mqn8WQylUiM40gkUsnk1fbU3GsWQCt7uyCZTk/ggEddvzmsxH6nKjoEaEQNdVi2TRUx8uCgJ+NJU7IjwHSlNklVdBjQDze6KdkZaf26HvubpqhnUFWw0TRJFZFqjnoGPUn0apqkyava1IOCfpiw0TRFb+qxAEVRb6Bhu9xsixpUx1FvoLXxpC1mQ/RF+VeKHa830Mor3RbTDP1z5c/Hg6H8AFWcNIloUKY4hfEC+mnaPjfboidTDwR6OGOfm+0c/R57iAzlB2ASUJXmKOoaNHqVctTEkdyJ0vN0Dxr57JScjRTdmKLn6RY0OCg5G6PoA4CWE87JaUZq5QFA5V3n5Gyk6PVU/J5Bw644HwS00qcP6qm54/cOWk674QTQ3TI9Tzeg7rLzQUDLDo1QF+jVPYNGJtxxAujN1H2OobxrTtLm3mdTFHVVbBugK5P32hTduOXEJXfuHrtc95oYVOMHO1EDXXHPCWPopzl6ngNBn7vnhKZo7/W9ZajiyTMk0PQcALrtutqSSNZoruc6g34Y3NlaQOt/0/N0BNU8VFsSOaprf06glWlPnJChUZpb0pxAI944oeTSHEKdQD1q0q64TqBXXqotjgTViusE6qnaEs896V5AVa/pCaDNBKWzANgX9M5btcUxXaHp2R90f8Yj58TETOU+trZEvHOGhKyZoJrhL6jn9IQE3TETtDDrL+iG1/SEBH1ONUH7gNqkp9BYl3eQJgkq5hClwmsPOkR6QktkbmuZRZKfoBWvoycOnW6C2oM2gHLYVc2FQopihIWwAW8ZmtBF2nolhEXi+Qx7imEVv+RFDaetqKk8L4r4A7jxWZV8Uw3zIlRn8kXXoC+8pyck6PavbU9pbL4Aj5JUmMcPcI8fyAf4Ff6iBN/A75Avugbd856ekKCHf/gO2loZ4nKhHJcNCaKqGHAH78CjStYBm18RuBYoWSUyPcUqeokEMYhKB6gqinm0gJBhQOZmkRhGl29QVtTwewu8Ag+Ce1D3K0NWULKoQDylVfQWrUkSWjtA65J0jNYRKs7DJwU0JqHbt2genq0fobfSLHy05gF0mPScmNiluaXFHtRoWvEq3sApCFyOeApZSNQcL2uhUFZT4JWgGFlOa31dbuenysuYD+VlDWki0uRSiTcOiGdQlSPVEl8twUd8GD4Ko36LS72gJ4O3qthFstbOT8jHeUy3Ko2heQn+1pakhif+bHUJ/40h8qmECq5Lbn04T32f9o5/PaCtWqqKUGU5QYkIKi+IRlYW1GAoLBqCrBmcYkTyAM0ZzaUHUnAbnrwh5IEO/nhkiCgol6otTzEnLFTlah4gIT1DeQEZrsfQocotLrhP2/V2/ngNexbGpKNZeJCO11ueUvF4a11axdRj8+h47RgVXY+hQ5VbHwpuD2h75TbH5SOCrGQ1AWSDoiyAp6BqAiAaQS0vRrJCR70NGXLTk1+4ND3D2FN+86wKFdZ41qi31dBClc+hKsrDPd65rP/ibzeo17WEpufKVLveHq0Rz3niuX60BHnY9ERbx0vrUhEtHa1CuS0eHx/3y89eUA8rtxbPa9cF1/UGNitoe21IiChyPqvxCvEMR0LYMxsGz6wG1TbLKZ3jJ1kjAk8ZGh8U5LWmp5pFQdwPQQGGfORDb2TIVvnSUDVZVOE93qkhtoLW3Ayfdik83fCUcAmVCi1PeIb7IfAEP6l4JOFsXS/iRgh/SXJqiK2gUTeTTztyfa7lGc/E44HNTXvK+PLp+VCgart31WRIxghMR8FTNDTe9MznOCEv5HKcwlnyUyCeqLqwEEYl5U0rP/lL9OxZHndH+UsEhIqAqnweHRwsaPzLl7nQgtOM1QJ61z18JhPjSWKcIs+ScJ/YsfFMVojn1tZ64XZp9m3DE81Kb9HREe6OttZuERAWoT+C/Dw6Wi9IW7fF4rqDpxV0v3v41NMTOnkvTZ7pcJ/esPNsDqDx5Qv05TzzbdEmD79txs/Ru+WhMrRjKR4KK2Qg9Lt4/FQFGWYaQo6LhFROhtRV82JYENv+ZFFey+VyobBWKmk5MZjDwGHjIBjOIlXMlvLwjqhUBQ2MlWB4AXpfoVpSHFcgOkG3uzyT5ZVkGfe8qZPIVbIcjVReJKY5O8/a67GxQrFYPC4UVlcLMCoWC/g2D8NoAVjhzbHimDS7tFYA4+OxwirufZdW+w6fPaDdS/H6/oa+X9bxABm51iuyXPmc1rlXvRmq102l+Du0fHZ+kXkCnvE4eY/ck+enm5nFJ5lA84PBpbcDNNc1uxTM+YkQav6Rm9Dxqvll8u8vQuB7vPBH5pli6E2QV5HWepM8QNqKL0ti4+suQXe6amky8mKai8K7qWh0ZZpLJH9w8JBK9uy+kDpp74/bUUSl4gHMOSFVm2+SuSgqStLWkmT55kDQwy4pvfx8JlKGJkmPlDdmuN2Z59znmciN3rMbp/7dPFopjnD6xePY893p4lkgcHYKFTb+7vQdpG5g88nH5TN4sQhfXQ6cng8SbYOGhg2h38pAFR1cdiWhqFweHJTcLQ22QbvbW+K5XUmmtu+w53gieVd7z91Fo92rgqntPhvNVtHBbbGLbfYWHSwNkOwB7W5viedeWU/v1Svg+SqtH+7PRD/IkeuuL6YPyUYzqKcm65PFzOLX5a8oHoACC88/np+eZdDZ8seL08DXj+dfv8AHF4uDS28LdIjF24an1sdHhJ6n+yO79waCdm/JJp4Jbnw6Ok48xxM/yu+5T+/vuhunRD9Pya7nsX1zAKidJzfBvZopX5ue6Y3yTPT7zGH3smBzwrKMMg3PeDyTAcEzFID+6NtiJhPIIFJvzxD+YDOAljMuRtLfYn55jhph3tGzdvJv+X3E9PxUec8lEjvd60iNFT/68eipk+fuhw+v5Jmy6fm8DPU2fdO9jtT2jDc9A6cXFyB4ik4DgU30z3kcEOPgeY4u4IPzABqMCfH4D8d6S5iFjvveb/Srt50h2jxzy2lfb8dfRGufpolnarpy8h7Gzx/dnn3rbWdINs/cctp73txw9UPieZOeqdfx+HndvdG7UW8BbrMxfma+PcEZGcicPbmABugdOm95QsJCR+TK85c/bPqhtpXCw/s5kVdxsyva7o7isC7QCq01OQkveObk/u325MDzarrMJZO4EfrxoxaF/rayE+1uhFMngz0L7cmJ4zTFltO2H+JuZipcWseee8/r0BNFKxuV7kY4/d30zHz8J54JLEJ/m/nveQZ0zwAUxTczMIMxPQHyPIOLshvPFqf9rkOajJ05Iwd/4ZxqW5T7rat3htranKb1Xbbty8l96ma6u0rewaD5KUGe1WrbiUTqZKfWs4tR6m7w8WZ4gagRyOFr9pw985X092v9w2564zBNntXr8CT94XP9sGeeWm+Ohf+gC/QVe75DT76hs3P0ES1mvlx8+W+AeH6Lx5fRxy/IlWebs7UcbwklD56GLAjZIMxVcrZFWbAcbSaKuOExmx6x9ZDFnqQVcunZydm7HJ8ifwnzIZVM4fxNJVI9S0TmgnwrSMNjNj1S64F4mq/ceXZy9i7Hp2GqQu7xqpBuzlPS6d6jztoL8pmzzQCeewbiZ5uQqvHA5lk8kNnEZThjTkbx6pH5yj0nl7UdHQUuh1f9QgYvGFnZfoeUzn3CxLzyEpXCbxCeY5YQWgjDWwjlES+ql+iZKrrztHC6W++zi6Tl6MHj4y20PraOtgB1DaH1wph0jNAaKBbeoqNZyZ2nhZMrD7d5xbLeRy06OfvsGN/hqajhiN0gq3Tmp1hFRhjcNGTwpUstuPCSN1A2/BI8kSIryJ2nlZOLDrkeP56Y7MzOVVQcO0KzY6gIjoWxrVtpHs2DL1lKmEfu8tPKyXHDLcdD0N4A2sXZZ8KCPUm9DUO9zcuqzTeyneOnuJDneXx7mZORIopZJJaqPDyAZVbNItWNZxcnx9kutrvg/NE5XcEbOMlta03CiwlQaNfWyQOwzhcwtHdO7zv3mZHe+5O252Mrp21DhFffc6QfUhQ1Z9g1wdZ9qsFTxDfiqQINCleh9IInWbGvai48ezi5T8N5Ji3tUMtzaU1CeB8xVMCbsMGziLYg5l149nB63Ze6Ge12yC9Ou52pBY0X+aygiLwh5ML28xXrLtUWT8hPaGxLCzB24vyUcUM02LOXc9gB1Dp8dnreHpv5+Zb0Q/PIbI8GevZyDjuAUh8+ezhtd9cUBKG9Nm+/npDtk58hvnSgqpclGD8V9RkeP0vhrDDY04aTs9t2MjiaO2za1Ns1qLBv8Q4nxflbGD+PVguF48GeNpwc93mo/cEo77Bpw+n54JWGuPXcGCIkM/S4cDPgdnmZhzY3dPlShZqrLTx7UxLD1SE4e7aYuUvPmnX2WQSyYhH63CL0t7e3q9DmFm/fzq9Cf7t1dAsPA1bjbTmH24GIdrm14/R0rGArjK5znTS2mgWt80+x/dJ59cGek4sOUXATVw7nOmmuveMJaeul83qfPSfH7Q6Rnq/odre2nEMlKN0jzPpx9i4RDZGeI0Y/zmE6Ir1O9VSNfTiHSdDu9PSJc4gETVxNDkaiwAkJ6vl4JPc7g43C2WfNzzE96R4v2JeT4068gtI9HMmB0/sBSfo+zdGzP6fnnRQoH6DtwOnx8GzqB2g7cHpeU9CpHi7oxMmJ3iou3TNiOHJyUU8jKFRbmidQcOT0WHHTr2ieQMGRs9+qfJ+genT2AE6Oq3lZxU2W6WkO5PR2jJlO84w1AzhhCPUAKlPc0WQgJwyh7kGn9ymePWEgJ0xC3Q+hM/sUT7Q5kNPFufuaIdA8YbULTg+nxJiuxe6VEyYtbkFn6hQPXHHB6T5D750TQN1l6HSN4m5grjhdg+p1ittVXHG6PI1C7p6LrRl3A86uiSORqtxvsTWj3ufMt52R1vfvOztxDD6Fn2DcayvUjsr4oDY3uULzJNWuOTmufDOoK9KvyzTHzj9d/zR50CAafiBOjov8cEzRRPKE5rKQB06IPccUTeuHcxTPUO2Bk3M+QTUk5/0tI/RG5arvaTYT0/9OUT09tSdO+GnXfUXTlE9P7Y0TQsvZ74AiGHQv8eCVE6K2YieaSE3vVOieQ8ojJ8T+Z9uTVKf1jf0YzYt2eObk8OU6urdgC0KO9gU7huDk8OU6EknL7pmJVHJ8e4qq5jCc+KftTViPJkvr+u5h5TXNiwEMxYkjqCpkHwUSOUOTaV9QZzhO8u+2/SKRbMb4zkmF9gV1huPEsf/9c1pvxsTG933aF9QZlpNEpLFROhKhOUUZlZNEtFLDUZmapHwpnZE4zX+1yn69Xq/tz8Wkv2lf8GokThJ+XY9uNE4Sfl2PbjROEuR6dPSvSDc6p1/Xi6TA6df1Iilw+nS9SAqcPl3PlQanT9dzpcHpjycNTn+ut0yF05/rLVPh9MWTCqcv10Onw+nL9dDpcPrhSYfTB09anD540uJs9kOjhD+cHD9yiD5xcr+OHE994uRe/2fU6NoaQ4uTQlhXlcLyQ/+ejrBev44eJ4WIPWaUk+OZ5bR6MsVp9WSJ0+LJFqfFky1OiydTnJ2ejHF2ejLG2enJFmeHJ2ucHZ6scXZ4MsbZ9mSOs+3JHGfbkzXOlid7nC1P9jhbnsxxNj0Z5Gx6MsjZ9GSPs+HJImfDk0XOhieDnKYnk5ymJ5OcpieLnMSTTU7iySYn8WSSE3syyok9GeXEnmxygiernODJKicX+4VRTo5nlhM8WeXkYn8xysnx1DaQUY8pZjm5mNtDju49Bly8/iEj+tA/oH9MPvQP+Bk/42fQjv8BY+lrKHxNUj8AAAAASUVORK5CYII=)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOAtaRG9e7Po"
      },
      "source": [
        "## **Steps necessary to create a language model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm23xy4sgLKW"
      },
      "source": [
        "- **Tokenization::** Convert the text into a list of words.\n",
        "\n",
        "- **Numericalization::** Make a list of all of the unique words that appear `(the vocab)`, and convert each word into a number, by looking up its index in the vocab.\n",
        "\n",
        "- **Language model data loader creation::** fastai provides an `LMDataLoader` class which automatically handles creating a dependent variable that is offset from the independent variable by one token. \n",
        "  - It also handles some important details, such as how to shuffle the training data in such a way that the dependent and independent variables maintain their structure as required.\n",
        "\n",
        "- **Language model creation::** we will be using a recurrent neural network `(RNN)` - for now, you can think of it as just another deep neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SZEmfrIharG"
      },
      "source": [
        "## **Text Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUXlIb4whloi"
      },
      "source": [
        "### **Tokenization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZ88oD-dis6v"
      },
      "source": [
        "**There are three main approaches to tokenization:**\n",
        "\n",
        "- **Word-based::** Split a sentence on spaces, as well as applying language-specific rules to try to separate parts of meaning even when there are no spaces (such as turning `\"don't\"` into `\"do n't\"`). \n",
        "\n",
        "  - Generally, punctuation marks are also split into separate tokens.\n",
        "\n",
        "- **Subword based::** Split words into smaller parts, based on the most commonly occurring substrings. \n",
        "\n",
        "  - For instance, **\"occasion\"** might be tokenized as `\"o c ca sion.\"`\n",
        "\n",
        "- **Character-based::** Split a sentence into its individual characters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUm5ikllj85m"
      },
      "source": [
        "### **Word Tokenization with fastai**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xh91D2c7j_LX"
      },
      "source": [
        "**Fastai provides a consistent interface to a range of tokenizers in external libraries.**\n",
        "- The default English word tokenizer for fastai uses a library called `spaCy`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLM4H4mAkd9j"
      },
      "source": [
        "#### **Let's try it out with the `IMDb` dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXG7lfTD9Wug"
      },
      "outputs": [],
      "source": [
        "from fastai.text.all import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "u0FPz_wskjVE",
        "outputId": "279a4b81-c029-443e-c8a4-418220c7dae8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='144441344' class='' max='144440600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [144441344/144440600 00:06&lt;00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "path = untar_data(URLs.IMDB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqYbauYokrz4"
      },
      "source": [
        "**We'll need to grab the text files in order to try out a tokenizer**. \n",
        "\n",
        "- Just like `get_image_files`, which gets all the **image files** in a path, `get_text_files` gets all the **text** files in a path. \n",
        "\n",
        "- We can also optionally pass `folders` to restrict the search to a particular list of subfolders:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okqz_39sk9CR"
      },
      "outputs": [],
      "source": [
        "files = get_text_files(path, folders = ['train', 'test', 'unsup'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJs85r_KlHrI"
      },
      "source": [
        "**Here's a review that we'll tokenize (we'll just print the start of it here to save space):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "5u-VHypDlJV9",
        "outputId": "00451ce3-f261-47f0-e9f9-22c1e091a205"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'When \"Woodstock\" occurred, I was a 15 y.o French teenager. Watching the fil'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "txt = files[0].open().read()\n",
        "\n",
        "txt[:75]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ducLU4znlfAr"
      },
      "source": [
        "**NOTE:**\n",
        "\n",
        "- Rather than directly using `SpacyTokenizer`, however, we'll use `WordTokenizer`.\n",
        "\n",
        "- This will always point to fastai's current default word tokenizer (which may not necessarily be `spaCy`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rlPOgRkmE7t"
      },
      "source": [
        "**We'll use fastai's `coll_repr(collection, n)` function to display the results.**\n",
        "\n",
        "- This displays the first `n` items of `collection`, along with the full size\n",
        "\n",
        "\n",
        "**Note:** Fastai's tokenizers take a `collection` of documents to tokenize, so we have to wrap `txt` in a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9g1DgKFWmv3v",
        "outputId": "f52d3590-540e-48b1-fa4d-a1b64df93862"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(#421) ['When','\"','Woodstock','\"','occurred',',','I','was','a','15','y.o','French','teenager','.','Watching','the','film','again',',','yesterday',',','I',\"'ve\",'been','stunned','by','its','quality',',','its'...]\n"
          ]
        }
      ],
      "source": [
        "spacy = WordTokenizer()\n",
        "\n",
        "toks = first(spacy([txt]))\n",
        "\n",
        "print(coll_repr(toks, 30))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ28OPIXoUC9"
      },
      "source": [
        "**NOTE:**\n",
        "\n",
        "-  SpaCY has a sophisticated rules engine with special rules for URLs, individual special English words, and much more. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQwq3_2Uojk2",
        "outputId": "58a26b9d-8afb-45b0-f09d-838dc561e29e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(#5) ['The','U.S.','dollar','1.00','.']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "first(spacy(['The U.S. dollar 1.00.']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJXpQdQRookZ"
      },
      "source": [
        "**Here we see that `\".\"` is separated when it terminates a sentence, but not in an acronym or number:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKpSuxhhoy52"
      },
      "source": [
        "**fastai then adds some additional functionality to the tokenization process with the `Tokenizer` class:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g94Q_U3Ro14W",
        "outputId": "f159e65c-be5f-4cb2-f0dd-292cee4c0824"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(#450) ['xxbos','xxmaj','when','\"','woodstock','\"','occurred',',','i','was','a','15','y.o','xxmaj','french','teenager','.','xxmaj','watching','the','film','again',',','yesterday',',','xxmaj','i',\"'ve\",'been','stunned','by'...]\n"
          ]
        }
      ],
      "source": [
        "tkn = Tokenizer(spacy)\n",
        "print(coll_repr(tkn(txt), 31))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahPN271-pHxY"
      },
      "source": [
        "**Notice that there are now some tokens that start with the characters \"xx\", which is not a common word prefix in English. `These are special tokens`.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1r4oEe8pLe3"
      },
      "source": [
        "- **E.g, the first item in the list, `xxbos`, is a special token that indicates the start of a new text (`\"BOS\"` is a standard NLP acronym that means `\"beginning of stream\"`).**\n",
        "\n",
        "  - By recognizing this start token, the model will be able to learn it needs to \"forget\" what was said previously and focus on upcoming words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXHB-rT8qQ0H"
      },
      "source": [
        "**Here are some of the main special tokens you'll see:**\n",
        "\n",
        "- **`xxbos`::** Indicates the beginning of a text (here, a review)\n",
        "- **`xxmaj`::** Indicates the next word begins with a capital (since we lowercased everything)\n",
        "- **`xxunk`::** Indicates the word is unknown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ogp-R9jKqqUe"
      },
      "source": [
        "**To see the rules that were used, you can check the default rules:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JISc7dyjqr4p",
        "outputId": "780453c9-bba0-40ff-dcc2-4b0536f80180"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<function fastai.text.core.fix_html(x)>,\n",
              " <function fastai.text.core.replace_rep(t)>,\n",
              " <function fastai.text.core.replace_wrep(t)>,\n",
              " <function fastai.text.core.spec_add_spaces(t)>,\n",
              " <function fastai.text.core.rm_useless_spaces(t)>,\n",
              " <function fastai.text.core.replace_all_caps(t)>,\n",
              " <function fastai.text.core.replace_maj(t)>,\n",
              " <function fastai.text.core.lowercase(t, add_bos=True, add_eos=False)>]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "defaults.text_proc_rules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXrbfzRjq2YU"
      },
      "source": [
        "**Here is a brief summary of what each does:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zn2s6R9Iq3nI"
      },
      "source": [
        "- **`fix_html`::** Replaces special HTML characters with a readable version (IMDb reviews have quite a few of these)\n",
        "\n",
        "- `replace_rep`:: Replaces any character repeated three times or more with a special token for repetition (`xxrep`), the number of times it's repeated, then the character.\n",
        "\n",
        "- `replace_wrep`:: Replaces any word repeated three times or more with a special token for word repetition (`xxwrep`), the number of times it's repeated, then the word.\n",
        "\n",
        "- `spec_add_spaces`:: Adds spaces around / and #\n",
        "\n",
        "- `rm_useless_spaces`:: Removes all repetitions of the space character\n",
        "\n",
        "- `replace_all_caps`:: Lowercases a word written in all caps and adds a special token for all caps (`xxup`) in front of it.\n",
        "\n",
        "- `replace_maj`:: Lowercases a capitalized word and adds a special token for capitalized (`xxmaj`) in front of it.\n",
        "\n",
        "- `lowercase`:: Lowercases all text and adds a special token at the beginning (`xxbos`) and/or the end (`xxeos`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGUlGpETrpY7"
      },
      "source": [
        "**Let's take a look at a few of them in action:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "GpPEqT9rrqiH",
        "outputId": "c902463e-5959-4ca1-de75-f7b23f630c47"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"(#11) ['xxbos','©','xxmaj','fast.ai','xxrep','3','w','.fast.ai','/','xxup','index']\""
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "coll_repr(tkn('©   Fast.ai www.fast.ai/INDEX'), 31)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOeC5b1Mr0hk"
      },
      "source": [
        "### **Numericalization with fastai**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwgdRsger3CM"
      },
      "source": [
        "**Numericalization** is the process of mapping tokens to integers. The steps are basically identical to those necessary to create a Category variable.\n",
        "\n",
        "1. Make a list of all possible levels of that categorical variable (the vocab).\n",
        "\n",
        "2. Replace each level with its index in the vocab. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA9iwYA1uQNN"
      },
      "source": [
        "**For our corpus, we'll use the first 2,000 movie reviews:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQ8ZgIxztuIg"
      },
      "outputs": [],
      "source": [
        "txts = files[0].open().read() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zekcoJ1u0AF"
      },
      "source": [
        "**Note:** Tokenization takes a while, it's done in parallel by fastai; but for this manual walkthrough, we'll use a small subset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocVLkARFu1kv",
        "outputId": "a2eba2ab-6c99-4e66-9e5c-b8e27b70fb1c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(#450) ['xxbos','xxmaj','when','\"','woodstock','\"','occurred',',','i','was'...]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "toks200 = L(txts).map(tkn)\n",
        "toks200[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfwV3Y_nulZm"
      },
      "source": [
        "We need to call `setup` on `Numericalize`; this is how we create the vocab. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "FwRkOPS0vJln",
        "outputId": "bdde65b9-b5ad-4b98-ff4b-23e1df6906d2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'(#40) [\\'xxunk\\',\\'xxpad\\',\\'xxbos\\',\\'xxeos\\',\\'xxfld\\',\\'xxrep\\',\\'xxwrep\\',\\'xxup\\',\\'xxmaj\\',\\'the\\',\\'-\\',\\',\\',\\'\"\\',\\'.\\',\\'and\\',\\'a\\',\\'of\\',\\'in\\',\"\\'s\",\\'(\\'...]'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num = Numericalize()\n",
        "num.setup(toks200)\n",
        "coll_repr(num.vocab,20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFZ6tr_Ovb2E"
      },
      "source": [
        "**Our special rules tokens appear first, and then every word appears once, in frequency order.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_58_LK2XvqVE"
      },
      "source": [
        "**NOTE:** Once we've created our `Numericalize` object, we can use it as if it were a function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O73U-a6Zv99l"
      },
      "outputs": [],
      "source": [
        "toks = tkn(txts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHNFYD-ovdMe",
        "outputId": "292f8a84-e303-43bb-c06b-04a05f2ad3fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorText([ 2,  8, 26, 12,  0, 12,  0, 11, 24,  0, 15,  0,  0,  8,  0,  0, 13,\n",
              "             8,  0,  9])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nums = num(toks)[:20]\n",
        "\n",
        "nums"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEwS3xtewQfK"
      },
      "source": [
        "This time, our tokens have been converted to a tensor of integers that our model can receive. \n",
        "\n",
        "- We can check that they map back to the original text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "OK_6sb_WwVOM",
        "outputId": "12032924-184a-4f9c-bc8c-9378b22eb580"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'xxbos xxmaj when \" xxunk \" xxunk , i xxunk a xxunk xxunk xxmaj xxunk xxunk . xxmaj xxunk the'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "' '.join(num.vocab[o] for o in nums)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIYaDy1IzTVR"
      },
      "source": [
        "## **Language Model Using DataBlock**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkK13-2q0FOu"
      },
      "source": [
        "**fastai** handles `tokenization` and `numericalization` automatically when `TextBlock` is passed to `DataBlock`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu2cUfr3X8nX"
      },
      "source": [
        "### **Get Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "_AzIikcSX8nf",
        "outputId": "875ae465-f424-4c46-e96b-99a996a32ded"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='144441344' class='' max='144440600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [144441344/144440600 00:02&lt;00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "path = untar_data(URLs.IMDB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjDjc_RoX8nk"
      },
      "source": [
        "NOTE: `get_text_files` gets all the text files in a path. We can also optionally pass `folders` to restrict the search to a particular list of subfolders:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HS3Dr0TWX8nn",
        "outputId": "4ca02cf1-a75c-4001-cdd0-148c406ca0c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(#7) [Path('/root/.fastai/data/imdb/tmp_clas'),Path('/root/.fastai/data/imdb/unsup'),Path('/root/.fastai/data/imdb/train'),Path('/root/.fastai/data/imdb/tmp_lm'),Path('/root/.fastai/data/imdb/imdb.vocab'),Path('/root/.fastai/data/imdb/test'),Path('/root/.fastai/data/imdb/README')]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "path.ls()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_L5vjimX8pq"
      },
      "outputs": [],
      "source": [
        "get_imdb = partial(get_text_files, folders = ['train', 'test', 'unsup'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "7sULJrDkX8qD",
        "outputId": "3e5e4b34-308b-45d3-8ce3-108df73e9e6f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dls_lm = DataBlock(\n",
        "    blocks = TextBlock.from_folder(path, is_lm=True),\n",
        "    get_items = get_imdb, \n",
        "    splitter = RandomSplitter(0.1)\n",
        ")\n",
        "\n",
        "dls = dls_lm.dataloaders(path, path=path, bs=128, seq_len=80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEBjeZUd4EJI"
      },
      "source": [
        "`TextBlock` is special,such that setting up the numericalizer's vocab can take a long time (we have to read and tokenize every document to get the vocab).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtfcQxPs4HeD"
      },
      "source": [
        "**To be as efficient as possible it performs a few optimizations:**\n",
        "\n",
        "- It saves the tokenized documents in a temporary folder, so it doesn't have to tokenize them more than once\n",
        "\n",
        "- It runs multiple tokenization processes in parallel, to take advantage of your computer's CPUs/GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMQzk6Gq4O8I"
      },
      "source": [
        "**We need to tell `TextBlock` how to access the texts, so that it can do this initial preprocessing—that's what `from_folder` does**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "Ld86IUmdAHo1",
        "outputId": "ad3a1ced-d559-4e84-9c8f-7e32378cada9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>xxbos xxmaj what an absolutely wonderful entertaining movie . xxmaj xxunk a great movie for the whole family and very wonderful characters in it . xxmaj what is wrong with the xxmaj hollywood people whom were not interested in this movie . i guess they are after all the digitally mastered movies that capture the awe of people . xxmaj you ca n't get much better than this movie with all stunts and no digital generation . i rather enjoy</td>\n",
              "      <td>xxmaj what an absolutely wonderful entertaining movie . xxmaj xxunk a great movie for the whole family and very wonderful characters in it . xxmaj what is wrong with the xxmaj hollywood people whom were not interested in this movie . i guess they are after all the digitally mastered movies that capture the awe of people . xxmaj you ca n't get much better than this movie with all stunts and no digital generation . i rather enjoy all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>well . xxmaj half the film appears to be padding , there 's a five minute sequence in which xxmaj germy walks around a funfair for absolutely no reason whatsoever , among other pointlessly drawn out scenes . xxmaj the ending is also one of the worst ever , nothing is concluded . xxmaj one to avoid . xxmaj this does have a great tag line on the video cover though , ' you 'll pay to get in …</td>\n",
              "      <td>. xxmaj half the film appears to be padding , there 's a five minute sequence in which xxmaj germy walks around a funfair for absolutely no reason whatsoever , among other pointlessly drawn out scenes . xxmaj the ending is also one of the worst ever , nothing is concluded . xxmaj one to avoid . xxmaj this does have a great tag line on the video cover though , ' you 'll pay to get in … and</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dls_lm.show_batch(max_n=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSFWB2PoX8qH",
        "outputId": "45958203-2fa7-4b45-c131-bea9ae8e17cb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>xxbos xxmaj i 've just revisited this fondly remembered bit of cinematic madness from my early days , and must urge you to beg steal or borrow it . \\n\\n xxmaj the story begins with a duel between a righteous xxmaj shaolin priest and our villain xxmaj abbot xxmaj white , needless to say , xxmaj abbot xxmaj white kicks xxmaj buddhist ass , and wages his campaign against xxmaj shaolin unhindered with the aid of his new ninja allies</td>\n",
              "      <td>xxmaj i 've just revisited this fondly remembered bit of cinematic madness from my early days , and must urge you to beg steal or borrow it . \\n\\n xxmaj the story begins with a duel between a righteous xxmaj shaolin priest and our villain xxmaj abbot xxmaj white , needless to say , xxmaj abbot xxmaj white kicks xxmaj buddhist ass , and wages his campaign against xxmaj shaolin unhindered with the aid of his new ninja allies (</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>xxmaj after a few times , i get bored and changed the channel . i still love xxmaj robert xxmaj culp and xxmaj patricia xxmaj crowley and xxmaj ray xxmaj milland in their roles but the story was weaker in this episode than in the others . xxmaj first , xxmaj robert xxmaj culp plays an investigator for xxmaj ray xxmaj milland 's character . xxmaj he hires him to investigate his young pretty wife played by xxmaj patricia xxmaj</td>\n",
              "      <td>after a few times , i get bored and changed the channel . i still love xxmaj robert xxmaj culp and xxmaj patricia xxmaj crowley and xxmaj ray xxmaj milland in their roles but the story was weaker in this episode than in the others . xxmaj first , xxmaj robert xxmaj culp plays an investigator for xxmaj ray xxmaj milland 's character . xxmaj he hires him to investigate his young pretty wife played by xxmaj patricia xxmaj crowley</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dls_lm.show_batch(max_n=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi7-w7DiX8qP"
      },
      "source": [
        "**Now that our data is ready, we can fine-tune the pretrained language model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjGoMA0qX8qR"
      },
      "source": [
        "### **Fine-Tuning the Language Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "PYb6hnKMX8qS",
        "outputId": "be161d86-ff11-441f-9c2c-bda390df7687"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='105070592' class='' max='105067061' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [105070592/105067061 00:01&lt;00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "learn = language_model_learner(\n",
        "    dls, AWD_LSTM, drop_mult=0.3,\n",
        "    metrics = [accuracy, Perplexity()]\n",
        ").to_fp16()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "fT1nOIOiANun",
        "outputId": "25aeb392-1f05-4f37-ba0e-83ba17dc87e3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>4.175750</td>\n",
              "      <td>4.064047</td>\n",
              "      <td>0.290021</td>\n",
              "      <td>58.209427</td>\n",
              "      <td>29:48</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "learn.fit_one_cycle(1, 1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pU6df6vxX8qf",
        "outputId": "ca51b8a4-e06e-4c1e-977b-f4a6cdd97d82"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>4.004746</td>\n",
              "      <td>3.900990</td>\n",
              "      <td>0.300720</td>\n",
              "      <td>49.451393</td>\n",
              "      <td>34:16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "learn.fit_one_cycle(1, 2e-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqV2Fow2X8qr"
      },
      "source": [
        "**Save the Model state**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZR4l_meX8qy"
      },
      "outputs": [],
      "source": [
        "learn.save('1epoch')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaG7o2gNX8qz"
      },
      "source": [
        "**Load the Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vtwb-jBAX8q1"
      },
      "outputs": [],
      "source": [
        "# learn = learn.load('1epoch')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QtBQyReX8q3"
      },
      "source": [
        "**Continue Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pBKSfWP2X8q4"
      },
      "outputs": [],
      "source": [
        "learn.unfreeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "60q_JB-YpiMr",
        "outputId": "5ccbe5b4-b8c4-4f30-d899-959a55d09e29"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='3' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      60.00% [3/5 1:34:23&lt;1:02:55]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.876920</td>\n",
              "      <td>3.814292</td>\n",
              "      <td>0.314378</td>\n",
              "      <td>45.344639</td>\n",
              "      <td>31:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.732242</td>\n",
              "      <td>3.684947</td>\n",
              "      <td>0.326720</td>\n",
              "      <td>39.843021</td>\n",
              "      <td>31:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.599468</td>\n",
              "      <td>3.610120</td>\n",
              "      <td>0.334572</td>\n",
              "      <td>36.970501</td>\n",
              "      <td>31:51</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "      <progress value='1787' class='' max='2629' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      67.97% [1787/2629 20:04&lt;09:27 3.5014]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='3' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      60.00% [3/5 1:34:23&lt;1:02:55]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.876920</td>\n",
              "      <td>3.814292</td>\n",
              "      <td>0.314378</td>\n",
              "      <td>45.344639</td>\n",
              "      <td>31:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.732242</td>\n",
              "      <td>3.684947</td>\n",
              "      <td>0.326720</td>\n",
              "      <td>39.843021</td>\n",
              "      <td>31:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.599468</td>\n",
              "      <td>3.610120</td>\n",
              "      <td>0.334572</td>\n",
              "      <td>36.970501</td>\n",
              "      <td>31:51</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "      <progress value='2482' class='' max='2629' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      94.41% [2482/2629 27:50&lt;01:38 3.4679]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "learn.fit_one_cycle(5,2e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdE9CR2xX8q5",
        "outputId": "d4142b4e-f864-4a87-9838-bf2a1cb84a51",
        "scrolled": false
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.766863</td>\n",
              "      <td>3.752828</td>\n",
              "      <td>0.317781</td>\n",
              "      <td>42.641506</td>\n",
              "      <td>36:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.669888</td>\n",
              "      <td>3.663725</td>\n",
              "      <td>0.327585</td>\n",
              "      <td>39.006378</td>\n",
              "      <td>36:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.533029</td>\n",
              "      <td>3.604167</td>\n",
              "      <td>0.335046</td>\n",
              "      <td>36.751049</td>\n",
              "      <td>36:16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.452684</td>\n",
              "      <td>3.572528</td>\n",
              "      <td>0.339135</td>\n",
              "      <td>35.606495</td>\n",
              "      <td>36:13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.371913</td>\n",
              "      <td>3.572680</td>\n",
              "      <td>0.339655</td>\n",
              "      <td>35.611897</td>\n",
              "      <td>36:14</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "learn.fit_one_cycle(5,2e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSn3CxsP5hQ2"
      },
      "source": [
        "- Once this is done, we save all of our model except the final layer that converts activations to probabilities of picking each token in our vocabulary. \n",
        "\n",
        "- The model not including the final layer is called the **encoder**. We can save it with `save_encoder`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yET7KQ8G5qeT"
      },
      "outputs": [],
      "source": [
        "# learn.save_encoder('finetuned')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvdZpIJtX8q_"
      },
      "outputs": [],
      "source": [
        "learn.export('/notebooks/FASTAI_2022/review_generator.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xVyTfTKX8rB"
      },
      "source": [
        "# **Text Generation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvVdd1bd50zZ"
      },
      "source": [
        "**Before moving on to fine-tuning the classifier, we can use our model to generate random reviews**\n",
        "\n",
        "- Since it's trained to guess what the next word of the sentence is, we can use the model to write new reviews:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngxgj6uXX8rB",
        "outputId": "152d5dda-2d4f-422b-b3d8-7e02a4b017d3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "TEXT = \"I like this movie because\"\n",
        "N_WORDS = 50\n",
        "N_SENTENCES = 2\n",
        "predicts = [learn.predict(TEXT, N_WORDS, temperature = 0.75) \n",
        "            for _ in range(N_SENTENCES)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "parGi94BX8rD",
        "outputId": "795470e8-7ec9-449c-ed03-9e12b93fb879"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i like this movie because it has the ability to make you feel clever and clever . The way the characters are portrayed is clever and i can not help but feel they are in one movie . i also like that this movie is also a movie that makes you feel good .\n",
            "i like this movie because it has a little bit of a twist , but it 's a very good movie . I 'm a big fan of the Jay Leno & Letterman show . i am really impressed . i watch every show on TV & the networks keep\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\".join(predicts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjSCNjWuX8rF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "vg5-QwVv8Un8",
        "FVlaMFIke0D5",
        "mPoylUKTevlD",
        "SOAtaRG9e7Po",
        "fUXlIb4whloi"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}